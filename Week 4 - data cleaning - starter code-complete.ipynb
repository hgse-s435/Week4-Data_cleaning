{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graphs inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we are going to work with some text data. In this folder, you should a text file called 'fullpapers.txt'. This file was generated by converting the proceedings of the EDM (Educational Data Mining) conference of 2018. You can find the proceedings here: http://educationaldatamining.org/EDM2017/proc_files/fullpapers.pdf\n",
    "We are going to explore the different terms that are used by authors of the papers in this conference, which will require some data cleaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is compare the different papers in terms of the vocabulary used. \n",
    "* open the pdf of the proceedings (fullpapers.pdf); \n",
    "* open the txt of the proceedigs (fullpapers.txt)\n",
    "\n",
    "1) we want to split the data into different papers. Brainstorm a few ideas on how to do that:\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) First we are going to read the fullpapers.txt file \n",
    "# and assign its content to a variable called \"data\"\n",
    "# hint: https://stackoverflow.com/questions/3758147/easiest-way-to-read-write-a-files-content-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3) To facilitate data processing, we want to split this file\n",
    "# into different pages. Create a list called \"pages\" that \n",
    "# stores the text presented on each page of the pdf\n",
    "# Look into the .split() function, what string are we going to want to split by?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) because we don't want to deal with upper case / lower case issues\n",
    "# we are going to lower case everything:\n",
    "# Try using a list comprehension to accomplish this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now we would like to join pages if they below to the same paper. Can you think of keywords we could like for to decided if the current page is starting a new paper? Write down two ideas:\n",
    "1. idea\n",
    "2. idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) create a new list called \"papers\", which is going to contain \n",
    "# all the papers we have. Iterate through all the pages and \n",
    "# add a new element to the list when you have a full paper\n",
    "# Using a for loop to iterate over all the pages, try to think of a conditional statement to check whether a page\n",
    "# represents a new 'paper'. I.e. what is a common aspect of all papers? \n",
    "papers = []\n",
    "current_paper = ''\n",
    "\n",
    "# iterate through the pages and add each paper to the list \"papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) print how many files you have in the \"papers\" list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) print the content of the first two paper to make sure it worked\n",
    "# (only print the first 300 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) create a new folder called papers; this is where we are \n",
    "# going to save each paper into a separate text file\n",
    "# hint: google \"how to create a new folder with python\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) save each paper into its unique file in the \"Papers\" folder\n",
    "# we created above\n",
    "# Hint: \"enumerate\" can provide you with the index of the paper in the list\n",
    "# Feel free to use the following filename for the first paper in the list:\n",
    "# ./Papers/paper0.txt on mac and .\\Papers\\paper0.txt on windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking yourself why we need to save the data into text files (instead of just using the list of papers above). One answer is that when we work with large datastsets, it's useful to save snapshots of our data that is \"clean\". This way we don't have to re-run all the code above and we save time. It also allows us to share data between different notebooks for other types of analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) We are going to practice your \"glob\" skills - find all the \n",
    "# text files in the \"Papers\" folder with a glob command!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) iterate through each of the text files and read their contents in the variable below:\n",
    "# Using a for loop, iterate over all the files in the directory, and add them to the list below\n",
    "text_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Now we are going to compute the frequency of each word across all \n",
    "# documents. Feel free to use the link below to help you!\n",
    "# hint: https://www.datacamp.com/community/tutorials/absolute-weighted-word-frequency\n",
    "# (look at the first block of code in the article)\n",
    "# Using the text_list we create in the cell above, iterate over all words and count their frequencies\n",
    "# If uncomfortable with dictionaries, google python dict\n",
    "word_freq = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) If you haven't done so already, create a dataframe from the dictionary\n",
    "# and print the head of the dataframe\n",
    "# Just as we did last week with Pandas, we can do this in only a few lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a problem with the dataframe above? Is there data meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) We are going to remove the following stop words, so that we see more interesting \n",
    "# keywors. Feel free to use the list and hint below to help you:\n",
    "# hint: https://stackoverflow.com/questions/43716402/remove-row-index-dataframe-pandas\n",
    "# the .drop() function could prove useful here\n",
    "STOPWORDS = ['a','able','about','across','after','all','almost','also','am','among',\n",
    "             'an','and','any','are','as','at','be','because','been','but','by','can',\n",
    "             'cannot','could','dear','did','do','does','either','else','ever','every',\n",
    "             'for','from','get','got','had','has','have','he','her','hers','him','his',\n",
    "             'how','however','i','if','in','into','is','it','its','just','least','let',\n",
    "             'like','likely','may','me','might','most','must','my','neither','no','nor',\n",
    "           'not','of','off','often','on','only','or','other','our','own','rather','said',\n",
    "             'say','says','she','should','since','so','some','than','that','the','their',\n",
    "             'them','then','there','these','they','this','tis','to','too','twas','us',\n",
    "             'wants','was','we','were','what','when','where','which','while','who',\n",
    "             'whom','why','will','with','would','yet','you','your']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) print the top 20 words of your new dataframe: we can do this with a list slice \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 plot the top 20 results above as a histogram: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell from this historgram? What do EDM researchers seem to care about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are improvements you could add to our data cleaning process? Write at least three things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count word frequencies per paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the previous section gave us an overall description of the word frequency for all the papers, it would be interesting to look at each individual paper. This is what we are going to do below, by focusing on the top 30 terms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) save the top 30 words from the dataframe above \n",
    "# in a new variable called \"top_words\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) We are now going to construct a new dataframe where each row is a paper, \n",
    "# each column is one of the top 30 words used and each cell is a count of this word. \n",
    "# NOTE: make sure you add another field called \"text\" where you're going to store the \n",
    "# actual text of the paper. \n",
    "# Hint: build a list of dataframes (one for each papers), \n",
    "# and use the concat function from pandas to concatenate them!\n",
    "d = []\n",
    "\n",
    "for text in text_list:\n",
    "    dic = {}\n",
    "    dic['text'] = text\n",
    "    # iterate through the top words, add counts to the dictionary\n",
    "    # and append the results to the list above (d)\n",
    "\n",
    "# concatenate the list d into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) create a scatter plot of the words 'learning' and 'data'\n",
    "# what can you say from it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21) annotate each point with the index number of the dataframe\n",
    "# hint: https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/\n",
    "# plt.txt( ) is going to be helpful for us here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22) what are the two extreme papers, \n",
    "# i.e., papers with more occurences for each term on each axis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23) plot the histogram of the paper that had high counts of \"data\"\n",
    "# hint: https://stackoverflow.com/questions/52392728/create-a-histogram-based-on-one-row-of-a-dataframe\n",
    "# .loc is going to be helpful here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24) plot the histogram of the paper that had high counts of \"learning\"\n",
    "# .loc is going to be helpful here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25) what can you observe? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26) print the first 1000 characters of each paper. \n",
    "\n",
    "\n",
    "# Is your interpretation confirmed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to work with Regex formulas to extract part of the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27) we are going to work on the first paper to make sure that our \n",
    "# regex works. Just retrieve the text and assign it to a variable below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28) find the text between the words 'abstract' and \n",
    "# 'introduction' for the first paper using a regex\n",
    "# https://stackoverflow.com/questions/12736074/regex-matching-between-two-strings/12736203\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29) find the text between the words 'abstract' and \n",
    "# 'introduction' for the first paper using the .index() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30) add a new column namd \"abstract\" to the dataframe above \n",
    "# and initialize it with an empty string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now add the abstracts to each row of the dataframe using either\n",
    "# of the two methods above\n",
    "# Hint: https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31) print your abstracts (they should contain a lot of \\n = carriage return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32) clean the abstract column using the \"apply\" function with a lambda\n",
    "# Hint: https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.apply.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing documents using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33) now we are going to do something a little more advanced:'\n",
    "# we are going to compute the similarity between two texts\n",
    "# using a method called tf-idf (we'll talk more about it later)\n",
    "# Hint: https://stackoverflow.com/questions/43631533/similarity-between-two-text-documents-in-python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34) What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35) repeat the same procedure with the entire papers\n",
    "# Use the same logic as the previous cell, but use the text_list variable that we defined previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36) What are two documents that seem to be very similar?\n",
    "# print their abstract: \n",
    "# print the first 1000 characters of each paper. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37) what seems to be similar between them? \n",
    "\n",
    "# they both talk about analyzing questions and answers from students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free exploration (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- try to extract the names of the author\n",
    "- find a way to get the top words shared across two texts\n",
    "- use a regex (or any other method) to get the list of references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
